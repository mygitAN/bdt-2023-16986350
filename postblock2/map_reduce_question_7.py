# -*- coding: utf-8 -*-
"""Map_Reduce Question 7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wOLZtAIFnB8L8BiDYpf4WkYvkAJDByDq

Question 1
"""

# Required libraries
from itertools import groupby
from operator import itemgetter

# --- MAPPER FUNCTION ---
def mapper(doc_id, contents):
    """
    The mapper function reads each document, tokenizes it into words,
    and emits a sequence of (word, document ID) pairs.

    :param doc_id: The document identifier (e.g., 'D1', 'D2').
    :param contents: The actual content/text of the document.

    :yield: A sequence of (word, document ID) pairs.
    """

    # Splitting the content into individual words.
    words = contents.split()

    # For each word in the document, yield (word, doc_id).
    # This creates pairs of words associated with their document ID.
    for word in words:
        # Convert words to lowercase to ensure uniformity and ignore empty strings.
        if word:
            yield (word.lower(), doc_id)

# --- REDUCER FUNCTION ---
def reducer(word, doc_ids):
    """
    The reducer function takes all pairs for a given word, sorts the corresponding
    document IDs, and emits a (word, list of document IDs) pair.

    :param word: The word being processed.
    :param doc_ids: List of document IDs where the word is found.

    :yield: A pair consisting of the word and a sorted list of unique document IDs.
    """

    # Removing duplicates by converting the list of document IDs to a set and then back to a list.
    # Also, sorting the document IDs.
    sorted_unique_doc_ids = sorted(list(set(doc_ids)))

    yield (word, sorted_unique_doc_ids)

# --- RUN MAPREDUCE ---
def run(documents):
    """
    This function orchestrates the MapReduce process. It applies the mapper to each document,
    then sorts and groups the intermediate results by word, and finally applies the reducer.

    :param documents: Dictionary of documents with doc_id as the key and content as the value.

    :return: The final reduced result - the inverted index.
    """

    # Applying the mapper to each document and collecting the results.
    mapped_results = [output for doc_id, content in documents.items() for output in mapper(doc_id, content)]

    # Sorting the mapped results by word to group them in the next step.
    sorted_mapped_results = sorted(mapped_results, key=itemgetter(0))

    # Grouping the results by word and applying the reducer.
    reduced_results = [output for word, group in groupby(sorted_mapped_results, key=itemgetter(0))
                       for output in reducer(word, [doc_id for _, doc_id in group])]

    return reduced_results

# --- SAMPLE DOCUMENTS ---
documents = {
    'D1': 'the cat sat on the mat',
    'D2': 'the dog sat on the log'
}

# Using the run function to get the inverted index.
inverted_index = run(documents)

# Displaying the inverted index.
for word, doc_ids in inverted_index:
    print(word, doc_ids)

"""Question 2"""

import requests
import re
import nltk
from nltk.corpus import stopwords
from itertools import groupby
from operator import itemgetter

# Ensure the NLTK stopword dataset is downloaded
nltk.download('stopwords')

# Fetch the content of a URL
def fetch_text(url):
    response = requests.get(url)
    return response.text

# Cleaner Function
def cleaner(line):
    words = re.findall(r'[a-z\']+', line.lower())
    stop_words = set(stopwords.words('english'))
    for word in words:
        word = word.replace("'", '')
        if word not in stop_words:
            yield word

# Mapper Function
def mapper(doc_id, contents):
    words = cleaner(contents)
    for word in words:
        yield (word, doc_id)

# Reducer Function
def reducer(word, doc_ids):
    sorted_unique_doc_ids = sorted(list(set(doc_ids)))
    yield (word, sorted_unique_doc_ids)

# Run MapReduce
def run(documents):
    mapped_results = [output for doc_id, content in documents.items() for output in mapper(doc_id, content)]
    sorted_mapped_results = sorted(mapped_results, key=itemgetter(0))
    reduced_results = [output for word, group in groupby(sorted_mapped_results, key=itemgetter(0))
                       for output in reducer(word, [doc_id for _, doc_id in group])]
    return reduced_results

# Fetching the content of the two plays
romeo_and_juliet_text = fetch_text("http://www.gutenberg.org/files/1112/1112.txt")
king_lear_text = fetch_text("http://www.gutenberg.org/cache/epub/1128/pg1128.txt")

# Creating a dictionary of the two plays with URLs as identifiers
documents_shakespeare = {
    "http://www.gutenberg.org/files/1112/1112.txt": romeo_and_juliet_text,
    "http://www.gutenberg.org/cache/epub/1128/pg1128.txt": king_lear_text
}

# Calculating the inverted index for the two plays
inverted_index_shakespeare = run(documents_shakespeare)

# Displaying an excerpt of twenty words from the inverted index
for word, doc_ids in inverted_index_shakespeare[:20]:
    print(word, doc_ids)

"""Question 3"""

# Cleaner Function remains unchanged from before

# Modified Mapper Function to include word position
def mapper(doc_id, contents):
    """
    The mapper function reads each document, tokenizes and cleans it using the cleaner function,
    then emits a sequence of (word, (document ID, position)) pairs.

    :param doc_id: The document identifier (e.g., 'D1', 'D2').
    :param contents: The actual content/text of the document.

    :yield: A sequence of (word, (document ID, position)) pairs.
    """
    # Use cleaner to tokenize and clean content
    words = list(cleaner(contents))

    # Enumerate provides position of the word in the list
    for position, word in enumerate(words):
        yield (word, (doc_id, position))

# Modified Reducer to collate word positions across documents
def reducer(word, doc_positions):
    # The reducer is modified to assemble all (doc_id, position) pairs for a given word
    collated_positions = [pos for pos in doc_positions]

    # Sorting primarily by doc_id and then by position for easy readability and access
    sorted_collated_positions = sorted(collated_positions, key=lambda x: (x[0], x[1]))

    yield (word, sorted_collated_positions)

# Rest of the code (cleaner and run functions) remain unchanged

# Fetching the content of the two plays, if not already fetched
# You can skip this if you've done it previously
romeo_and_juliet_text = fetch_text("http://www.gutenberg.org/files/1112/1112.txt")
king_lear_text = fetch_text("http://www.gutenberg.org/cache/epub/1128/pg1128.txt")

# Constructing the inverted index with word positions for the two plays
inverted_index_positions = run({
    "http://www.gutenberg.org/files/1112/1112.txt": romeo_and_juliet_text,
    "http://www.gutenberg.org/cache/epub/1128/pg1128.txt": king_lear_text
})

# Displaying an excerpt of twenty words from the inverted index with positions
for word, doc_positions in inverted_index_positions[:20]:
    print(word, doc_positions)

