{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAt9AMKK7dA8",
        "outputId": "9818cf5a-8d06-46ba-c5fa-992e788b6083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.2 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n"
          ]
        }
      ],
      "source": [
        "!lsb_release -a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pHuqNnT7iRM",
        "outputId": "badd6ba6-7c9b-4f31-87c2-bc4c07e9724e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,231 kB]\n",
            "Fetched 2,482 kB in 4s (685 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9Q_doWLDa5"
      },
      "source": [
        "Install Java, as Spark depends on it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSxsoRsNHlC1"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKu4VHE9K3Ty",
        "outputId": "23ae4560-9895-4c1f-b03d-4d28ad9c209e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-04 17:15:10--  https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz.2’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M   198MB/s    in 1.9s    \n",
            "\n",
            "2023-11-04 17:15:12 (198 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz.2’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get spark\n",
        "VERSION='3.5.0'\n",
        "!wget https://dlcdn.apache.org/spark/spark-$VERSION/spark-$VERSION-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnUd2Hk1KcCF"
      },
      "source": [
        "Download Spark and decompress it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuVTO57TKKTg"
      },
      "outputs": [],
      "source": [
        "# decompress spark\n",
        "!tar xf spark-$VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# install python package to help with system paths\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLr6LR8rLXOo"
      },
      "source": [
        "Set up environment variables for Java and Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2IjBqFELYw_"
      },
      "outputs": [],
      "source": [
        "# Let Colab know where the java and spark folders are\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{VERSION}-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxnrvS9SLqBA"
      },
      "source": [
        "Initialize Spark using findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ImoU9lTLqWS"
      },
      "outputs": [],
      "source": [
        "# add pyspark to sys.path using findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkYwHF6VMAOd"
      },
      "source": [
        "Create a Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLnnTFg6MAb1"
      },
      "outputs": [],
      "source": [
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-bYrZ5jMa1G"
      },
      "source": [
        "Download the dataset directly to the environment and read it using Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F8hhrcnMKwv",
        "outputId": "c08bbefa-3ba6-4388-ac2e-c9bc821162a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-04 17:15:25--  https://storage.googleapis.com/bdt-demand-forecast/sales-data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.114.207, 172.217.212.207, 108.177.111.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.114.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17333449 (17M) [text/csv]\n",
            "Saving to: ‘gcs_sales_data.csv’\n",
            "\n",
            "gcs_sales_data.csv  100%[===================>]  16.53M  17.3MB/s    in 1.0s    \n",
            "\n",
            "2023-11-04 17:15:27 (17.3 MB/s) - ‘gcs_sales_data.csv’ saved [17333449/17333449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/bdt-demand-forecast/sales-data.csv -O gcs_sales_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFaB1sxdMZOv"
      },
      "source": [
        "Read the data from the downloaded CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwSx7QiAMNfI"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('gcs_sales_data.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnmtezY9NKvA",
        "outputId": "793f7ea4-59c4-4175-866b-5b0ed946c82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+-----+\n",
            "|      date|store|item|sales|\n",
            "+----------+-----+----+-----+\n",
            "|2013-01-01|    1|   1|   13|\n",
            "|2013-01-02|    1|   1|   11|\n",
            "|2013-01-03|    1|   1|   14|\n",
            "|2013-01-04|    1|   1|   13|\n",
            "|2013-01-05|    1|   1|   10|\n",
            "|2013-01-06|    1|   1|   12|\n",
            "|2013-01-07|    1|   1|   10|\n",
            "|2013-01-08|    1|   1|    9|\n",
            "|2013-01-09|    1|   1|   12|\n",
            "|2013-01-10|    1|   1|    9|\n",
            "|2013-01-11|    1|   1|    9|\n",
            "|2013-01-12|    1|   1|    7|\n",
            "|2013-01-13|    1|   1|   10|\n",
            "|2013-01-14|    1|   1|   12|\n",
            "|2013-01-15|    1|   1|    5|\n",
            "|2013-01-16|    1|   1|    7|\n",
            "|2013-01-17|    1|   1|   16|\n",
            "|2013-01-18|    1|   1|    7|\n",
            "|2013-01-19|    1|   1|   18|\n",
            "|2013-01-20|    1|   1|   15|\n",
            "+----------+-----+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0ZAzsvF9jts"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGlXPWW4G4bP"
      },
      "source": [
        "Prepare Data by Partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-deLRf5CoYL"
      },
      "outputs": [],
      "source": [
        "# Prepare Data: Repartition based on 'store' and 'item' for parallel processing\n",
        "store_item_history = df.repartition(col(\"store\"), col(\"item\"))\n",
        "\n",
        "# Convert the 'date' column to string format\n",
        "store_item_history = store_item_history.withColumn(\"date\", col(\"date\").cast(\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uihMhiJhEDJT",
        "outputId": "8bcd69ac-42d4-4310-96fb-5a5d711d0ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+-----+\n",
            "|      date|store|item|sales|\n",
            "+----------+-----+----+-----+\n",
            "|2013-01-01|    3|  22|   50|\n",
            "|2013-01-02|    3|  22|   53|\n",
            "|2013-01-03|    3|  22|   51|\n",
            "+----------+-----+----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "store_item_history.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ8_wMS4G-Ze"
      },
      "source": [
        "Model Fit & Forecast for Each Store-Item Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwWQevHyEXVu"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.types import IntegerType, StringType, StructField, StructType, DoubleType\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Adjust the output schema to include a 'forecast' field\n",
        "schema = StructType([\n",
        "    StructField(\"date\", StringType()),  # 'date' to a StringType or else fails\n",
        "    StructField(\"store\", IntegerType()),\n",
        "    StructField(\"item\", IntegerType()),\n",
        "    StructField(\"sales\", IntegerType()),  # Historical sales\n",
        "    StructField(\"forecast\", DoubleType())  # Forecasted sales\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def prophet_fit_forecast_function(pdf):\n",
        "    # Fit the model\n",
        "    model = Prophet(daily_seasonality=True)\n",
        "    model.fit(pdf.rename(columns={'date': 'ds', 'sales': 'y'}))\n",
        "\n",
        "    # Create a future dataframe for forecasting\n",
        "    future = model.make_future_dataframe(periods=0)  # Can always adjust periods if forecasting future beyond the historical data\n",
        "\n",
        "    # Forecast\n",
        "    forecast = model.predict(future)\n",
        "\n",
        "    # Assign the forecasted 'yhat' to a new 'forecast' column\n",
        "    pdf['forecast'] = forecast['yhat'].values\n",
        "\n",
        "    # Ensure we return the DataFrame with the structure defined in the schema\n",
        "    pdf['date'] = pdf['date'].astype(str)  # Convert date back to string if necessary\n",
        "\n",
        "    return pdf[['date', 'store', 'item', 'sales', 'forecast']]  # Return the structured DataFrame\n",
        "\n",
        "# Apply the UDF to forecast\n",
        "store_item_forecasts = store_item_history.groupBy('store', 'item').apply(prophet_fit_forecast_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr0RMR6yHRut"
      },
      "source": [
        "Persist Forecasts for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk3jC9YyEjIu"
      },
      "outputs": [],
      "source": [
        "path_to_save = \"forecasts.csv\"\n",
        "store_item_forecasts.write.mode(\"overwrite\").csv(path_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVMsfv_hIBTP"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrmY1EhXGHgM"
      },
      "outputs": [],
      "source": [
        "def evaluate_forecast(evaluation_pd):\n",
        "    mae = mean_absolute_error(evaluation_pd['sales'], evaluation_pd['forecast'])\n",
        "    mse = mean_squared_error(evaluation_pd['sales'], evaluation_pd['forecast'])\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    results = {'mae':[mae], 'mse':[mse], 'rmse':[rmse]}\n",
        "    return pd.DataFrame.from_dict(results)\n",
        "\n",
        "evaluation_results = store_item_forecasts.groupBy('store', 'item').applyInPandas(evaluate_forecast, schema=\"mae float, mse float, rmse float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfDbA1gmIIHW"
      },
      "source": [
        "Print Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq8YBcLQGQ__",
        "outputId": "c6a71671-88f1-4983-b483-dfff862fdf2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+---------+\n",
            "|      mae|      mse|     rmse|\n",
            "+---------+---------+---------+\n",
            "|3.5115125|19.698908| 4.438345|\n",
            "|6.1717906| 60.78164| 7.796258|\n",
            "| 4.682532|35.544044|5.9618826|\n",
            "|3.6469831|20.626503| 4.541641|\n",
            "|6.1389756| 59.68847|7.7258315|\n",
            "| 7.155143|  80.2021| 8.955563|\n",
            "|5.6625113|50.739956|7.1231985|\n",
            "| 6.642689|  69.9807| 8.365447|\n",
            "|3.9194186|24.434675|4.9431443|\n",
            "|5.1415143|41.410084|6.4350667|\n",
            "|5.2595453|44.609444|  6.67903|\n",
            "| 7.498411|88.230385| 9.393104|\n",
            "|  4.32822|29.853817| 5.463865|\n",
            "|7.0863786|78.689514| 8.870711|\n",
            "|5.4263525|46.505756| 6.819513|\n",
            "|3.6331272| 20.56307| 4.534652|\n",
            "|5.2866144|  43.9465|6.6292157|\n",
            "|6.6468644| 69.84013| 8.357041|\n",
            "| 6.970256| 77.73334| 8.816651|\n",
            "|4.3034377|29.081266| 5.392705|\n",
            "+---------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptU2Z3knLRsp"
      },
      "source": [
        "2) Number of partitions in the store_item_history dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-R3qi1YLQGl",
        "outputId": "1d1700cf-39b8-4b52-ef1a-920bca334684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of partitions: {store_item_history.rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc8jWIWh0f9b"
      },
      "source": [
        "3) Parallelise the workload and Demonstrate Parallisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cufG2JkDE8Ws",
        "outputId": "567ef371-7edb-4ace-9b75-9293ad651a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration without parallelization (single core): 1.471919298171997 seconds\n",
            "Duration with parallelization (all cores): 0.4441835880279541 seconds\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Function to time the operation\n",
        "def time_operation(spark_session, operation):\n",
        "    start_time = time.time()\n",
        "    operation(spark_session)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Create a Spark session with a single core\n",
        "spark_single_core = SparkSession.builder.master(\"local[1]\").appName(\"SingleCore\").getOrCreate()\n",
        "\n",
        "# Create another Spark session using all available cores\n",
        "spark_multi_core = SparkSession.builder.master(\"local[*]\").appName(\"MultiCore\").getOrCreate()\n",
        "\n",
        "# Define your DataFrame here (for the sake of an example, we're creating a simple DataFrame with dummy data)\n",
        "data = [(\"Store1\", \"Item1\", 1), (\"Store2\", \"Item2\", 2), (\"Store1\", \"Item1\", 3)]\n",
        "columns = [\"store\", \"item\", \"sales\"]\n",
        "df = spark_multi_core.createDataFrame(data, columns)\n",
        "\n",
        "# Define the operation you want to time\n",
        "def grouped_count(spark_session):\n",
        "    df.groupBy('store', 'item').count().collect()\n",
        "\n",
        "# Time the operation without parallelization (using a single core)\n",
        "non_parallel_duration = time_operation(spark_single_core, grouped_count)\n",
        "\n",
        "# Time the operation with parallelization (using all available cores)\n",
        "parallel_duration = time_operation(spark_multi_core, grouped_count)\n",
        "\n",
        "# Print the durations\n",
        "print(f\"Duration without parallelization (single core): {non_parallel_duration} seconds\")\n",
        "print(f\"Duration with parallelization (all cores): {parallel_duration} seconds\")\n",
        "\n",
        "# Stop the Spark sessions\n",
        "spark_single_core.stop()\n",
        "spark_multi_core.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}